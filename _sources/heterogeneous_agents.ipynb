{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Heterogeneous Agents \n",
    "Consider the environment in section 5. We need to solve the following:\n",
    "\\begin{align}\n",
    "    \\begin{split}\n",
    "    {L}^e_{HJB} \\left(v^e, v^h, \\kappa, \\chi;x \\right) &= \\frac{\\rho_e}{1-\\rho_e} \\delta_e^{1 / \\rho_e} \\exp\\left[({1- \\frac{1}{\\rho_e}})v^e)\\right]-\\frac{\\delta_e}{1-\\rho_e}+r\\\\&+\\frac{1}{2 \\gamma_e} \\frac{\\left(\\Delta^e+\\pi^h \\cdot \\sigma_R\\right)^2}{\\left\\|\\sigma_R\\right\\|^2}\n",
    "+\\left[\\mu_X+\\frac{1-\\gamma_e}{\\gamma_e}\\left(\\frac{\\Delta^e+\\pi^h \\cdot \\sigma_R}{\\left\\|\\sigma_R\\right\\|^2}\\right) \\sigma_X \\sigma_R\\right] \\cdot \\partial_X v^e \\\\\n",
    "&+\\frac{1}{2}\\left[\\operatorname{tr}\\left(\\sigma_X^{\\prime} \\partial_{xx^{\\prime}} v^e \\sigma_X\\right)+\\frac{1-\\gamma_e}{\\gamma_e}\\left(\\sigma_X^{\\prime} \\partial_x v_e\\right)^{\\prime}\\left[\\gamma_e \\mathbb{I}_d+\\left(1-\\gamma_e\\right) \\frac{\\sigma_R \\sigma_R^{\\prime}}{\\left\\|\\sigma_R\\right\\|^2}\\right] \\sigma_X^{\\prime} \\partial_x v^e\\right]= 0 \n",
    "\\end{split}\\\\\n",
    "\\begin{split}\n",
    "    {L}^h_{HJB} \\left(v^e, v^h, \\kappa, \\chi;x \\right) &=\\frac{\\rho_h}{1-\\rho_h} \\delta_h^{1 / \\rho_h}  \\exp\\left[({1- \\frac{1}{\\rho_h}})v^h)\\right]-\\frac{\\delta_h}{1-\\rho_h}+r+\\frac{1}{2 \\gamma_h}\\|\\pi^h\\|^2\\\\&+\\left[\\mu_X+\\frac{1-\\gamma_h}{\\gamma_h} \\sigma_X \\pi^h\\right] \\cdot \\partial_x v^h +\\frac{1}{2}\\left[\\operatorname{tr}\\left(\\sigma_X^{\\prime} \\partial_{xx^{\\prime}} v^h \\sigma_X\\right)+\\frac{1-\\gamma_h}{\\gamma_h}\\left\\|\\sigma_X^{\\prime} \\partial_x v^h\\right\\|^2\\right]=0\\end{split} \\\\\n",
    "\\begin{split}\n",
    "    {L}_{\\kappa} \\left(v^e, v^h,\\kappa, \\chi;x \\right) &= \\min\\Big\\{ 1 - \\kappa, \\, w\\gamma_h (1-\\chi\\kappa) | \\sigma_R |^2 - (1-w) \\gamma_e \\chi \\kappa | \\sigma_R |^2  \\\\\n",
    "\t\\qquad &+ w(1-w) \\frac{\\alpha_e - \\alpha_h}{\\underline{\\chi} q} + w(1-w) \\left( \\sigma_x \\sigma_R \\right) \\cdot \\left[ (\\gamma_h-1)\\partial_x \\upsilon^h -  (\\gamma_e-1)\\partial_x \\upsilon^e \\right] \\Big\\} = 0\\\\\n",
    "\\end{split} \\\\\n",
    "\\begin{split}\n",
    "    {L}_{\\chi} \\left(v^e, v^h, \\kappa, \\chi;x \\right) &= \\min\\Big\\{ \\chi - \\underline{\\chi}, \\, \\Big[ ((1-w)\\gamma_e + w\\gamma_h) | D_{z} |^2 + (\\partial_w \\log q) D_{\\upsilon,z} - D_{\\upsilon,w} \\Big](\\chi - w) \\\\\n",
    " \\quad &+ w(1-w) (\\gamma_e - \\gamma_h) | D_{z} |^2 - D_{\\upsilon,z} \\Big\\} = 0\n",
    "\\end{split}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{align}\n",
    " D_{z} &\\doteq \\sqrt{z_2}\\sigma_k + \\sigma_{z}' \\partial_{z} \\log q \\\\\n",
    " D_{\\upsilon,w} &\\doteq w(1-w) | D_{z} |^2 \\partial_w  \\big[ (\\gamma_h - 1) \\upsilon^h - (\\gamma_e - 1)\\upsilon^e \\big] \\\\\n",
    " D_{\\upsilon,z} &\\doteq w(1-w) \\left(\\sigma_{z}D_{z} \\right) \\cdot \\partial_{z} \\big[ (\\gamma_h - 1) \\upsilon^h - (\\gamma_e - 1)\\upsilon^e  \\big]\n",
    "\\end{align}\n",
    "\n",
    "Since $\\chi$ can be solved algebraically, we solve (1) to (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "We modify the `DeepGalerkinMethod` code from https://github.com/alialaradi/DeepGalerkinMethod. We construct an object `sim_NN` of class `DGMNet` with the following hyperparameters:\n",
    "\n",
    "```{list-table}\n",
    ":header-rows: 1\n",
    "\n",
    "* - Input\n",
    "  - Description\n",
    "  - Parameter used in paper\n",
    "* - `n_layers`\n",
    "  - Number of layers\n",
    "  - 2\n",
    "* - `units`\n",
    "  - Number of neurons in each layer\n",
    "  - 16\n",
    "* - `input_dim`\n",
    "  - Dimension of input into first layer\n",
    "  - 3 (This should be the same as the number of states)\n",
    "* - `activation`\n",
    "  - Activation function for all layers except the last\n",
    "  - tanh\n",
    "* - `final_activation`\n",
    "  - Activation function for final layer\n",
    "  - Identity function for first two dimensions; sigmoid for third dimension. This is so that...\n",
    "* - `seed`\n",
    "  - Seed for weight and bias initialization\n",
    "  - 256\n",
    "```\n",
    "\n",
    "We use a Glorot normal initializer to initialize weights and a Glorot uniform initializer to initialize the biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Training\n",
    "The training set is constructed by drawing uniformly from the three-dimensional cube bounded by [`wMin`,`zMin`,`vMin`] and [`wMax`,`zMax`,`vMax`]. The loss function is given by:\n",
    "\n",
    "$$\n",
    "{L}^e_{HJB} + {L}^h_{HJB} + p{L}_{\\kappa}\n",
    "$$\n",
    "\n",
    "Where $p$ is a penalization parameter. We compute the gradient using `tf.GradientTape` and use an `L-BFGS-B` solver. The full list of parameters for the training stage are:\n",
    "\n",
    "```{list-table}\n",
    ":header-rows: 1\n",
    "\n",
    "* - Input\n",
    "  - Description\n",
    "  - Parameter used in paper\n",
    "* - `points_size`\n",
    "  - Determines the `batchSize`, which is $2^x$ where $x$ is `points_size`. Batch size is the number of training samples in each epoch.\n",
    "  - 10\n",
    "* - `iter_num`\n",
    "  - Number of epochs, i.e. the number of complete passes through the training set.\n",
    "  - 5\n",
    "* - `penalization`\n",
    "  - Penalty for violating $\\kappa$ constraint\n",
    "  - 10000\n",
    "* - `seed`\n",
    "  - Seed for drawing uniform samples\n",
    "  - 256 (same as seed for initialization)\n",
    "* - `max_iter`\n",
    "  - Maximum number of L-BFGS-B iterations (number of times parameters are updated)\n",
    "  - 100\n",
    "* - `maxfun`\n",
    "  - Maximum number of function evaluations\n",
    "  - 100\n",
    "* - `gtol`\n",
    "  - Iteration will stop when $\\max|proj(g_i)| \\leq$ `gtol` for each entry $i$ of the (projected) gradient vector\n",
    "  - Machine epsilon\n",
    "* - `seed`\n",
    "  - Seed for weight and bias initialization\n",
    "  - 256\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "(10000 for the results in the paper)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
